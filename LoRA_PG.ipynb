{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPmVaSsykJ0lCo+3HnREOYf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mightyoctopus/lora-fine-tuning-example-code/blob/main/LoRA_PG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbjvVacdhtnZ"
      },
      "outputs": [],
      "source": [
        "!pip install -q peft transformers datasets torch accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mTVAiDP2C3yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: Rabe3/QA_Synthatic_Medical_data\n",
        "\n",
        "Model (Instrcut): Qwen/Qwen3-0.6B"
      ],
      "metadata": {
        "id": "K0_v6IsspPxw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- quantization\n",
        "- base model load and load peft model (with LoraConfig)\n",
        "- tokenization (batches)\n",
        "- load dataset\n",
        "- map dataset to be tokenized\n",
        "- Train (with TrainingArguments configued)\n"
      ],
      "metadata": {
        "id": "i9H00fg6IIfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig\n",
        "                          )\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "from google.colab import drive, userdata\n",
        "from huggingface_hub import login, snapshot_download\n",
        "\n",
        "import os"
      ],
      "metadata": {
        "id": "sZ-XVnacTV5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Qwen/Qwen3-0.6B\"\n",
        "medical_dataset_name = \"Rabe3/QA_Synthatic_Medical_data\"\n",
        "general_dataset_name = \"tatsu-lab/alpaca\""
      ],
      "metadata": {
        "id": "HGC1vc8jULUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\")\n",
        "cache_path = \"/content/drive/My Drive/models/huggingface_cache\""
      ],
      "metadata": {
        "id": "95_wGZQjhRmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "model_folder_name = \"models--\" + model_name.replace(\"/\", \"--\")\n",
        "parent_model_path_in_drive = os.path.join(cache_path, model_folder_name)\n",
        "\n",
        "if not os.path.exists(parent_model_path_in_drive):\n",
        "    print(\"Model not found in Drive -- downloading from HF...\")\n",
        "    model_path = snapshot_download(\n",
        "        repo_id=model_name,\n",
        "        cache_dir=cache_path,\n",
        "        local_dir_use_symlinks=False\n",
        "    )\n",
        "else:\n",
        "    print(\"Model found in Drive -- fetching from the cache...\")\n",
        "    snapshots_dir = os.path.join(parent_model_path_in_drive, \"snapshots\")\n",
        "    drive_id = os.listdir(snapshots_dir)\n",
        "\n",
        "    if drive_id:\n",
        "        model_path = os.path.join(snapshots_dir, drive_id[0])\n",
        "    else:\n",
        "        raise ValueError(\"No snapshot found in the cache path.\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    cache_dir=cache_path\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n"
      ],
      "metadata": {
        "id": "-ldCmwciZjf5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r = 8,\n",
        "    lora_alpha=16,\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "pwQl8kt6eHkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_medical = load_dataset(medical_dataset_name, \"default\", split=\"train[:2000]\")\n",
        "### Add general dataset as extra for better generalization of the model\n",
        "general_dataset =load_dataset(general_dataset_name, split=\"train\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JRUiY5aCGCfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take 10â€“20% of general dataset for balance\n",
        "general_ratio = 0.15\n",
        "g_count = round(len(data_medical) * general_ratio / (1 - general_ratio))\n",
        "\n",
        "# Shuffle and sample\n",
        "data_general = general_dataset.shuffle(seed=42).select(range(min(g_count, len(general_dataset))))\n",
        "\n",
        "# Merge both\n",
        "data_mixed = concatenate_datasets([data_medical, data_general]).shuffle(seed=42)\n",
        "\n",
        "print(f\"Medical: {len(data_medical)}, General: {len(data_general)}, Mixed: {len(data_mixed)}\")\n"
      ],
      "metadata": {
        "id": "VY8ZumtjZl2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(batch):\n",
        "    texts = []\n",
        "\n",
        "    for convo in batch[\"conversations\"]:\n",
        "        for turn in convo:\n",
        "            human_msg = turn[\"value\"] if turn[\"from\"] == \"human\" else \"\"\n",
        "            assisant_msg = turn[\"value\"] if turn[\"from\"] == \"gpt\" else \"\"\n",
        "\n",
        "            texts.append(f\"### Instruction:\\n{human_msg}\\n### Response:\\n{assisant_msg}\")\n",
        "\n",
        "\n",
        "    tokens = tokenizer(\n",
        "        texts,\n",
        "        padding=\"max_length\",\n",
        "        max_length=256,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    print(\"TOKENS\", tokens)\n",
        "\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "mS1K2W7HHTqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data = data_medical.map(tokenize, batched=True, remove_columns=['conversations', 'source', 'score', 'metadata'])"
      ],
      "metadata": {
        "id": "I2_TSU9UM9di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir = \"./fine_tuned_result\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate= 1e-5,\n",
        "    num_train_epochs=1,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    remove_unused_columns=False,\n",
        "    label_names=[\"labels\"],\n",
        "    report_to=\"none\",\n",
        "    save_total_limit=1\n",
        ")"
      ],
      "metadata": {
        "id": "9zKo68dHlNjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data,\n",
        "    processing_class=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "Mo-nS8CNswCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = trainer.train()"
      ],
      "metadata": {
        "id": "Bd_TYG63t2MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result.training_loss)"
      ],
      "metadata": {
        "id": "D9i-ORAM3UgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Get Perplexity:\n",
        "import math\n",
        "\n",
        "loss = result.training_loss\n",
        "perplexity = math.exp(loss)\n",
        "\n",
        "print(f\"Perplexity: {perplexity}\")"
      ],
      "metadata": {
        "id": "L06o83Ng2qK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Log in to Hugging Face to push the model\n",
        "user_token = userdata.get(\"HF_TOKEN\")\n",
        "login(user_token)\n",
        "\n",
        "\n",
        "model.push_to_hub(\"MightyOctopus/qwen3-0.6B-lora-medical\")\n",
        "tokenizer.push_to_hub(\"MightyOctopus/qwen3-0.6B-lora-medical\")\n"
      ],
      "metadata": {
        "id": "tVxuzXKQZHlZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}