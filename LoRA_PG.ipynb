{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNINN9ualVGRbyeyOEcrES3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mightyoctopus/lora-fine-tuning-example-code/blob/main/LoRA_PG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WbjvVacdhtnZ"
      },
      "outputs": [],
      "source": [
        "!pip install -q peft transformers datasets torch accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mTVAiDP2C3yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: Rabe3/QA_Synthatic_Medical_data\n",
        "\n",
        "Model (Instrcut): Qwen/Qwen3-0.6B"
      ],
      "metadata": {
        "id": "K0_v6IsspPxw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Load model\n",
        "- tokenizing\n",
        "- quantization\n",
        "- load dataset\n",
        "- Prepare Dataset Format\n",
        "\n",
        "tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "- Lora Config\n",
        "- train\n",
        "- Save the fine tuned model"
      ],
      "metadata": {
        "id": "i9H00fg6IIfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig\n",
        "                          )\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "from google.colab import drive, userdata\n",
        "from huggingface_hub import login, snapshot_download\n",
        "\n",
        "import os"
      ],
      "metadata": {
        "id": "sZ-XVnacTV5c"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Qwen/Qwen3-0.6B\"\n",
        "dataset_name = \"Rabe3/QA_Synthatic_Medical_data\""
      ],
      "metadata": {
        "id": "HGC1vc8jULUk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\")\n",
        "cache_path = \"/content/drive/My Drive/models/huggingface_cache\""
      ],
      "metadata": {
        "id": "95_wGZQjhRmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "model_folder_name = \"models--\" + model_name.replace(\"/\", \"--\")\n",
        "parent_model_path_in_drive = os.path.join(cache_path, model_folder_name)\n",
        "\n",
        "if not os.path.exists(parent_model_path_in_drive):\n",
        "    print(\"Model not found in Drive -- downloading from HF...\")\n",
        "    model_path = snapshot_download(\n",
        "        repo_id=model_name,\n",
        "        cache_dir=cache_path,\n",
        "        local_dir_use_symlinks=False\n",
        "    )\n",
        "else:\n",
        "    print(\"Model found in Drive -- fetching from the cache...\")\n",
        "    snapshots_dir = os.path.join(parent_model_path_in_drive, \"snapshots\")\n",
        "    drive_id = os.listdir(snapshots_dir)\n",
        "\n",
        "    if drive_id:\n",
        "        model_path = os.path.join(snapshots_dir, drive_id[0])\n",
        "    else:\n",
        "        raise ValueError(\"No snapshot found in the cache path.\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    cache_dir=cache_path\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n"
      ],
      "metadata": {
        "id": "-ldCmwciZjf5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r = 8,\n",
        "    lora_alpha=16,\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "pwQl8kt6eHkV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_dataset(dataset_name, \"default\", split=\"train[:200]\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JRUiY5aCGCfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(batch):\n",
        "\n",
        "    for convo in batch[\"conversations\"]:\n",
        "        for turn in convo:\n",
        "            human_msg = turn[\"value\"] if turn[\"from\"] == \"human\" else \"\"\n",
        "            assisant_msg = turn[\"value\"] if turn[\"from\"] == \"gpt\" else \"\"\n",
        "\n",
        "            texts = [\n",
        "                f\"### Instruction:\\n{human_msg}\\n### Response:\\n{assisant_msg}\"\n",
        "            ]\n",
        "\n",
        "    tokens = tokenizer(\n",
        "        texts,\n",
        "        padding=\"max_length\",\n",
        "        max_length=256,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "mS1K2W7HHTqR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data = data.map(tokenize, batched=True, remove_columns=data.column_names)"
      ],
      "metadata": {
        "id": "I2_TSU9UM9di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir = \"./fine_tuned_result\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate= 1e-3,\n",
        "    num_train_epochs=50,\n",
        "    fp16=True,\n",
        "    logging_steps=20,\n",
        "    save_strategy=\"epoch\",\n",
        "    remove_unused_columns=False,\n",
        "    label_names=[\"labels\"],\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "9zKo68dHlNjL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data,\n",
        "    processing_class=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "Mo-nS8CNswCl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = trainer.train()"
      ],
      "metadata": {
        "id": "Bd_TYG63t2MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result.training_loss)"
      ],
      "metadata": {
        "id": "D9i-ORAM3UgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Get Perplexity:\n",
        "import math\n",
        "\n",
        "loss = result.training_loss\n",
        "perplexity = math.exp(loss)\n",
        "\n",
        "print(f\"Perplexity: {perplexity}\")"
      ],
      "metadata": {
        "id": "L06o83Ng2qK1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}